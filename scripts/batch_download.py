"""
Batch download and process papers.
"""
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.scrapers.ganoderma_news import GanodermaScraper
from src.scrapers.pdf_downloader import EnhancedPDFDownloader
from src.processors.pdf_parser import PDFParser
from src.processors.text_chunker import TextChunker
from loguru import logger
import json
import time


def batch_download_papers(max_papers: int = 10):
    """Batch download papers."""
    logger.info("=" * 60)
    logger.info("æ‰¹æ¬¡ä¸‹è¼‰è«–æ–‡")
    logger.info("=" * 60)
    
    # Initialize
    scraper = GanodermaScraper()
    downloader = EnhancedPDFDownloader()
    parser = PDFParser()
    chunker = TextChunker(chunk_size=1000, chunk_overlap=200)
    
    # Step 1: Scrape papers
    logger.info("\næ­¥é©Ÿ 1: çˆ¬å–è«–æ–‡åˆ—è¡¨")
    
    categories = ['ç ”ç©¶æ–°çŸ¥']
    all_papers = []
    
    for category in categories:
        logger.info(f"çˆ¬å–åˆ†é¡: {category}")
        try:
            # Get article URLs from category page
            article_urls = scraper.scrape_category_page(category)
            logger.info(f"æ‰¾åˆ° {len(article_urls)} ç¯‡æ–‡ç« ")
            
            # Extract paper info from each article
            for article_url in article_urls[:15]:  # Limit to 15 articles
                try:
                    paper_info = scraper.extract_paper_links(article_url)
                    if paper_info:
                        all_papers.append(paper_info)
                        logger.success(f"âœ“ æå–è«–æ–‡: {paper_info.get('paper_source')}")
                    time.sleep(2)  # Be polite
                except Exception as e:
                    logger.warning(f"æå–å¤±æ•—: {e}")
            
            time.sleep(3)  # Be polite between categories
        except Exception as e:
            logger.error(f"âœ— çˆ¬å–å¤±æ•—: {e}")
    
    logger.info(f"\nç¸½å…±æ‰¾åˆ° {len(all_papers)} ç¯‡è«–æ–‡")
    
    # Step 2: Download PDFs
    logger.info("\næ­¥é©Ÿ 2: ä¸‹è¼‰ PDF")
    
    downloaded = []
    failed = []
    
    for i, paper in enumerate(all_papers[:max_papers], 1):
        paper_url = paper.get('paper_url')
        paper_source = paper.get('paper_source')
        paper_id = paper.get('paper_id')
        
        logger.info(f"\n[{i}/{min(max_papers, len(all_papers))}] {paper_id}")
        
        if not all([paper_url, paper_source, paper_id]):
            logger.warning("ç¼ºå°‘å¿…è¦è³‡è¨Šï¼Œè·³é")
            continue
        
        try:
            pdf_path = downloader.download_pdf(paper_url, paper_source, paper_id)
            
            if pdf_path:
                logger.success(f"âœ“ ä¸‹è¼‰æˆåŠŸ: {pdf_path}")
                downloaded.append({
                    **paper,
                    'pdf_path': pdf_path
                })
            else:
                logger.warning(f"âœ— ä¸‹è¼‰å¤±æ•—")
                failed.append(paper_id)
            
            time.sleep(3)  # Be polite
        
        except Exception as e:
            logger.error(f"âœ— éŒ¯èª¤: {e}")
            failed.append(paper_id)
    
    logger.info(f"\nä¸‹è¼‰çµ±è¨ˆ:")
    logger.info(f"  æˆåŠŸ: {len(downloaded)} ç¯‡")
    logger.info(f"  å¤±æ•—: {len(failed)} ç¯‡")
    
    # Step 3: Process PDFs
    logger.info("\næ­¥é©Ÿ 3: è™•ç† PDF")
    
    all_chunks = []
    
    for i, paper in enumerate(downloaded, 1):
        pdf_path = paper.get('pdf_path')
        paper_id = paper.get('paper_id')
        
        logger.info(f"\n[{i}/{len(downloaded)}] è™•ç† {paper_id}")
        
        try:
            # Parse PDF
            parsed = parser.parse_pdf(pdf_path)
            
            if not parsed:
                logger.warning("è§£æå¤±æ•—")
                continue
            
            # Chunk text
            if parsed['structure']:
                chunks = chunker.chunk_by_sections(
                    parsed['structure'],
                    metadata={
                        'paper_id': paper_id,
                        'file_name': parsed['file_name'],
                        'source_url': paper.get('article_url', ''),
                        'category': paper.get('category', ''),
                    }
                )
            else:
                chunks = chunker.chunk_text(
                    parsed['content'],
                    metadata={
                        'paper_id': paper_id,
                        'file_name': parsed['file_name'],
                        'source_url': paper.get('article_url', ''),
                        'category': paper.get('category', ''),
                    }
                )
            
            all_chunks.extend(chunks)
            logger.success(f"âœ“ è™•ç†æˆåŠŸ: {len(chunks)} å€‹åˆ†å¡Š")
        
        except Exception as e:
            logger.error(f"âœ— è™•ç†å¤±æ•—: {e}")
    
    # Step 4: Save all chunks
    logger.info("\næ­¥é©Ÿ 4: å„²å­˜åˆ†å¡Š")
    
    output_dir = Path("data/processed")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Combine with existing chunks
    existing_chunks_file = output_dir / "all_chunks.json"
    
    if existing_chunks_file.exists():
        with open(existing_chunks_file, 'r', encoding='utf-8') as f:
            existing_chunks = json.load(f)
        logger.info(f"è¼‰å…¥ç¾æœ‰åˆ†å¡Š: {len(existing_chunks)} å€‹")
        all_chunks.extend(existing_chunks)
    
    # Remove duplicates by paper_id
    seen_papers = set()
    unique_chunks = []
    
    for chunk in all_chunks:
        paper_id = chunk.get('paper_id', '')
        chunk_index = chunk.get('chunk_index', 0)
        key = f"{paper_id}_{chunk_index}"
        
        if key not in seen_papers:
            seen_papers.add(key)
            unique_chunks.append(chunk)
    
    # Save
    with open(existing_chunks_file, 'w', encoding='utf-8') as f:
        json.dump(unique_chunks, f, ensure_ascii=False, indent=2)
    
    logger.success(f"âœ“ å„²å­˜å®Œæˆ: {len(unique_chunks)} å€‹åˆ†å¡Š")
    
    # Summary
    logger.info("\n" + "=" * 60)
    logger.info("æ‰¹æ¬¡è™•ç†å®Œæˆ")
    logger.info("=" * 60)
    logger.info(f"çˆ¬å–è«–æ–‡: {len(all_papers)} ç¯‡")
    logger.info(f"ä¸‹è¼‰æˆåŠŸ: {len(downloaded)} ç¯‡")
    logger.info(f"è™•ç†æˆåŠŸ: {len(all_chunks)} å€‹æ–°åˆ†å¡Š")
    logger.info(f"ç¸½åˆ†å¡Šæ•¸: {len(unique_chunks)} å€‹")
    logger.info(f"å„²å­˜ä½ç½®: {existing_chunks_file}")
    
    return {
        'scraped': len(all_papers),
        'downloaded': len(downloaded),
        'processed': len(all_chunks),
        'total_chunks': len(unique_chunks)
    }


def main():
    """Run batch download."""
    logger.info("\n" + "ğŸ„ " * 20)
    logger.info("æ‰¹æ¬¡ä¸‹è¼‰éˆèŠè«–æ–‡")
    logger.info("ğŸ„ " * 20 + "\n")
    
    result = batch_download_papers(max_papers=10)
    
    logger.success("\nâœ“ æ‰¹æ¬¡ä¸‹è¼‰å®Œæˆï¼")
    logger.info(f"\nç¾åœ¨çŸ¥è­˜åº«æœ‰ {result['total_chunks']} å€‹åˆ†å¡Šå¯ç”¨")
    logger.info("é‡æ–°å•Ÿå‹• Web ä»‹é¢å³å¯ä½¿ç”¨æ–°çš„è«–æ–‡è³‡æ–™ï¼")


if __name__ == "__main__":
    main()
